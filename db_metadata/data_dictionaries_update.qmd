---
title: "Update Data Dictionaries"
author: "Cob Staines"
format: html
editor: source
---

```{r}
# librarian downloads, if not already downloaded, and reads in needed packages
librarian::shelf(tidyverse, dbplyr, DBI, RPostgres, here, janitor, RIBBiTR-BII/ribbitrrr)

## Connect to DB
dbcon <- hopToDB()

```

## Define functions to pull table and column metadata

```{r}
pull_pg_column_data <- function(schema) {
  col_query <- paste0("
    SELECT 
      c.table_schema,
      c.table_name,
      c.column_name,
      c.data_type,
      c.character_maximum_length,
      c.numeric_precision,
      c.datetime_precision,
      c.is_nullable,
      c.column_default,
      c.ordinal_position,
      pg_catalog.col_description(format('%s.%s',c.table_schema,c.table_name)::regclass::oid, c.ordinal_position) as pg_description
    FROM 
      information_schema.columns c
    WHERE 
      c.table_schema = '", schema, "'
    ORDER BY 
      c.table_name, c.ordinal_position
  ")
  
  columns <- tbl(dbcon, sql(col_query)) %>% collect()
  
  const_query <- paste0("
    SELECT tc.table_schema, tc.table_name, kcu.column_name, tc.constraint_name, tc.constraint_type
    FROM
      information_schema.key_column_usage kcu
    LEFT JOIN 
      information_schema.table_constraints tc
      ON kcu.table_schema = tc.table_schema
      AND kcu.table_name = tc.table_name
      AND kcu.constraint_name = tc.constraint_name
    WHERE 
      kcu.table_schema = '", schema, "'
  ")
  
  constraints <-tbl(dbcon, sql(const_query)) %>% collect()
  
  pkeys = constraints %>%
    filter(constraint_type == "PRIMARY KEY") %>%
    select(table_name,
           column_name) %>%
    mutate(primary_key = TRUE)
  
  fkeys = constraints %>%
    filter(constraint_type == "FOREIGN KEY") %>%
    select(table_name,
           column_name) %>%
    mutate(foreign_key = TRUE)
  
  unique = constraints %>%
    filter(constraint_type == "UNIQUE") %>%
    select(table_name,
           column_name) %>%
    mutate(unique = TRUE)
  
  columns_constraints = columns %>%
    left_join(pkeys, by = c("table_name", "column_name")) %>%
    left_join(fkeys, by = c("table_name", "column_name")) %>%
    left_join(unique, by = c("table_name", "column_name")) %>%
    mutate(primary_key = ifelse(is.na(primary_key), FALSE, TRUE),
           foreign_key = ifelse(is.na(foreign_key), FALSE, TRUE),
           unique = ifelse(is.na(unique), FALSE, TRUE),
           key_type = case_when(  # depreciate on Jan 2025
             primary_key ~ "PK",
             foreign_key ~ "FK",
             TRUE ~ NA_character_)) %>%
    select(table_schema,
           table_name,
           column_name,
           primary_key,
           foreign_key,
           unique,
           is_nullable,
           everything())
    
  
  return(columns_constraints)
}

pull_pg_table_data <- function(schema) {
  query <- paste0("
    SELECT 
      t.table_schema, t.table_name,
      (SELECT count(*) FROM information_schema.columns c WHERE c.table_name = t.table_name AND c.table_schema = t.table_schema) as column_count,
      pg_catalog.obj_description(format('%s.%s',t.table_schema,t.table_name)::regclass::oid, 'pg_class') as table_description
    FROM 
      information_schema.tables t
    WHERE 
      t.table_schema = '", schema, "'
  ")
  
  tables <- tbl(dbcon, sql(query))
  
  return(tables)
}

column_dict_supplementary = c(
  "definition",
  "units",
  "accuracy",
  "scale",
  "format",
  "reviewed",
  "natural_key"
)


dict_supplementary_mutate <- function(dict, supplementary) {
  for (new_cols in supplementary) {
    dict[,new_cols] <- NA
  }
  
  dict$reviewed = FALSE
  
  return(dict)
}

tables_pkey = c("table_schema", "table_name")
columns_pkey = c("table_schema", "table_name", "column_name")

```

## Pull Schemas

```{r}
schemas <- dbGetQuery(dbcon, "SELECT schema_name FROM information_schema.schemata
                      WHERE schema_name NOT LIKE 'pg_temp_%'
                      AND schema_name NOT LIKE 'pg_toast_temp_%'
                      AND schema_name != 'pg_catalog'
                      AND schema_name != 'information_schema'
                      AND schema_name != 'public';")$schema_name
```

## Pull pg data and metadata tables

```{r}
# define lists
pg_table_data = list()
pg_column_data = list()

meta_table_data = list()
meta_column_data = list()

# columns derived from postgres information_schema
meta_column_data_pg = list()
# supplementary columns derived from manual human input
meta_column_data_sup = list()

for (schema in schemas){
  # pull pg data from information_schema
  pg_table_data[[schema]] = pull_pg_table_data(schema)
  pg_column_data[[schema]] = pull_pg_column_data(schema)
  
  # pull metadata tables from each schema, dropping supplementary columns
  meta_table_data[[schema]] = tbl(dbcon, Id(schema, "metadata_tables"))
  meta_column_data[[schema]] = tbl(dbcon, Id(schema, "metadata_columns"))
  
  meta_column_data_pg[[schema]] = meta_column_data[[schema]] %>%
    select(-any_of(column_dict_supplementary))
  meta_column_data_sup[[schema]] = meta_column_data[[schema]] %>%
    select(any_of(columns_pkey),
           any_of(column_dict_supplementary))
  
  cat("Data pulled for schema:", schema, "\n")
}

```

## QA: Compare pg and metadata for diferences
check to make sure columns align between pg and metadata tables

```{r}
# QC
table_comp = list()
column_comp = list()
for (schema in schemas){
  if (!identical(colnames(meta_table_data[[schema]]), colnames(pg_table_data[[schema]]))) {
    stop(paste0("Columns in pg_table_data and meta_table_data do not align for schema '", schema))
  }
  
  if (!identical(colnames(meta_column_data_pg[[schema]]), colnames(pg_column_data[[schema]]))) {
    stop(paste0("Columns in pg_column_data and meta_column_data_pg do not align for schema '", schema))
  }
}

```

## Compare for staging

```{r}

meta_table_insert = list()
meta_table_orphan = list()
meta_table_update = list()
meta_table_duplicate = list()

meta_column_insert = list()
meta_column_orphan = list()
meta_column_update = list()
meta_column_duplicate = list()

for (schema in schemas) {
  table_results = compare_for_staging(meta_table_data[[schema]] %>%
                                       collect(),
                                      pg_table_data[[schema]] %>%
                                       collect(),
                                     tables_pkey,
                                     return_all= TRUE)
  meta_table_insert[[schema]] = table_results$insert
  meta_table_orphan[[schema]] = table_results$orphan
  meta_table_update[[schema]] = table_results$update
  meta_table_duplicate[[schema]] = table_results$duplicate
  
  column_results = compare_for_staging(meta_column_data_pg[[schema]] %>%
                                         collect(),
                                       pg_column_data[[schema]] %>%
                                         collect(),
                                       columns_pkey,
                                       return_all=TRUE)
  meta_column_insert[[schema]] = column_results$insert
  meta_column_orphan[[schema]] = column_results$orphan
  meta_column_update[[schema]] = column_results$update
  meta_column_duplicate[[schema]] = column_results$duplicate
  
  cat("Discrepancies processed for schema: ", schema, "\n")
}


```
## warnings for orphan data, flag for update or append

```{r}
update = FALSE
insert = FALSE

meta_table_upsert = list()
meta_column_upsert = list()

for (schema in schemas) {
  # warnings for orphan data
  if (nrow(meta_table_orphan[[schema]]) > 0) {
    warning(paste0("Table orphan data found in schema: ", schema, ".\n\t Investigate 'meta_table_orphan$", schema, "' and resolve manually.\n"))
  }
  if (nrow(meta_column_orphan[[schema]]) > 0) {
    warning(paste0("Column orphan data found in schema: ", schema, ".\n\t Investigate 'meta_column_orphan$", schema, "' and resolve manually.\n"))
  }
  
  # flag if append needed
  if (nrow(meta_table_insert[[schema]]) > 0) {
    insert = TRUE
    cat("Table inserts found for schema: ", schema, "\n")
  }
  if (nrow(meta_column_insert[[schema]]) > 0) {
    insert = TRUE
    cat("Column inserts found for schema: ", schema, "\n")
  }
  
  # flag if updates needed
  if (nrow(meta_table_update[[schema]]) > 0) {
    update = TRUE
    cat("Table updates found for schema: ", schema, "\n")
  }
  if (nrow(meta_column_update[[schema]]) > 0) {
    update = TRUE
    cat("Column updates found for schema: ", schema, "\n")
  }
  
  meta_table_upsert[[schema]] = rbind(meta_table_update[[schema]],
                                      meta_table_insert[[schema]])
  meta_column_upsert[[schema]] = rbind(meta_column_update[[schema]],
                                      meta_column_insert[[schema]])
  
}

cat("-------------\nUpdate: ", update, "\nInsert: ", insert, "\n")
```


## create temporary DB tables with upsert data, and upsert
```{r}
if (update || insert) {
  # begin transaction
  dbBegin(dbcon)
  
  tryCatch(
    {
      for (schema in schemas) {
        # some local test to avoid below if not needed
        if (nrow(meta_table_upsert[[schema]]) > 0) {
          # upsert table_data
          reference_table = meta_table_data[[schema]]
          # novel_data = pg_table_data[[schema]] %>% collect()
          novel_data = meta_table_upsert[[schema]]
          temp_table_name = stage_to_temp(dbcon, reference_table, novel_data)
          pointer = tbl(dbcon, temp_table_name)
          rows_upsert(meta_table_data[[schema]], pointer, by=tables_pkey, in_place=TRUE)
          
          cat("Tables upserted for schema: ", schema, "\n")
        }
        
        if (nrow(meta_column_upsert[[schema]]) > 0) {
          # upsert column data
          reference_table = meta_column_data[[schema]]
          # novel_data = pg_column_data[[schema]] %>% collect()
          novel_data = meta_column_upsert[[schema]]
          temp_table_name = stage_to_temp(dbcon, reference_table, novel_data)
          pointer = tbl(dbcon, temp_table_name)
          rows_upsert(meta_column_data[[schema]], pointer, by=columns_pkey, in_place=TRUE)
        
          cat("Columns upserted for schema: ", schema, "\n")
        }
      }
      
      # Commit the transaction if successful
      dbCommit(dbcon)
      print("Transaction successful! All tables are up to date.")

    }, error = function(e) {
      # Rollback in case of error
      dbRollback(dbcon)
      message("Transaction failed: ", e$message)
    })
}else{
  print("No new data. All tables are up to date.")
}

```

## drop orphans
```{r}
# does not work yet, tables don't share same source. Could use DBI instead.

# begin transaction
dbBegin(dbcon)

tryCatch(
  {
    for (schema in schemas) {
      # some local test to avoid below if not needed
      if (nrow(meta_table_orphan[[schema]]) > 0) {
        # drop orphans
        rows_delete(meta_table_data[[schema]], meta_table_orphan[[schema]], by=tables_pkey, in_place=TRUE)
        
        cat("Table orphans deleted for schema: ", schema, "\n")
      }
      
      if (nrow(meta_column_upsert[[schema]]) > 0) {
        # drop orphans
        rows_delete(meta_column_data[[schema]], meta_column_orphan[[schema]], by=columns_pkey, in_place=TRUE)
      
        cat("Column orphans deleted for schema: ", schema, "\n")
      }
    }
    
    # Commit the transaction if successful
    dbCommit(dbcon)
    print("Transaction successful! All tables are up to date.")

  }, error = function(e) {
    # Rollback in case of error
    dbRollback(dbcon)
    message("Transaction failed: ", e$message)
  })

```

```{r}
dbDisconnect(dbcon)
```