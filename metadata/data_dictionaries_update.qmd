---
title: "Update Data Dictionaries"
author: "Cob Staines"
format: html
editor: visual
---

```{r setup, include=FALSE}
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

# librarian downloads, if not already downloaded, and reads in needed packages
librarian::shelf(tidyverse, DBI, RPostgres, here, janitor)
```

## Connect to DB

```{r}
# establish connection
tryCatch({
  print("Connecting to Databaseâ€¦")
  con <- dbConnect(dbDriver("Postgres"),
                          dbname = Sys.getenv("aws_dbname"),
                          host = Sys.getenv("aws_host"),
                          port = Sys.getenv("aws_port"),
                          user = Sys.getenv("aws_user"),
                          password = Sys.getenv("aws_password"),
                          timezone=NULL)
  print("Database Connected!")
},
error=function(cond) {
  print("Unable to connect to Database.")
})

```

## Define functions to pull table and column metadata

```{r}
pull_pg_column_data <- function(schema) {
  query <- paste0("
    SELECT 
      c.table_schema,
      c.table_name,
      c.column_name,
      c.data_type,
      c.character_maximum_length,
      c.numeric_precision,
      c.datetime_precision,
      c.is_nullable,
      c.column_default,
      c.ordinal_position,
      pg_catalog.col_description(format('%s.%s',c.table_schema,c.table_name)::regclass::oid, c.ordinal_position) as pg_description,
      CASE 
        WHEN tc.constraint_type = 'PRIMARY KEY' THEN 'PK'
        WHEN tc.constraint_type = 'FOREIGN KEY' THEN 'FK'
        WHEN tc.constraint_type = 'UNIQUE' THEN 'UQ'
        ELSE NULL
      END as key_type
    FROM 
      information_schema.columns c
    LEFT JOIN 
      information_schema.key_column_usage kcu
      ON c.table_schema = kcu.table_schema
      AND c.table_name = kcu.table_name
      AND c.column_name = kcu.column_name
    LEFT JOIN 
      information_schema.table_constraints tc
      ON kcu.table_schema = tc.table_schema
      AND kcu.table_name = tc.table_name
      AND kcu.constraint_name = tc.constraint_name
    WHERE 
      c.table_schema = '", schema, "'
    ORDER BY 
      c.table_name, c.ordinal_position
  ")
  
  # columns <- dbGetQuery(con, query)
  columns <- tbl(con, sql(query))
  
  return(columns)
}

pull_pg_table_data <- function(schema) {
  query <- paste0("
    SELECT 
      t.table_schema, t.table_name,
      (SELECT count(*) FROM information_schema.columns c WHERE c.table_name = t.table_name AND c.table_schema = t.table_schema) as column_count,
      pg_catalog.obj_description(format('%s.%s',t.table_schema,t.table_name)::regclass::oid, 'pg_class') as table_description
    FROM 
      information_schema.tables t
    WHERE 
      t.table_schema = '", schema, "'
  ")
  
  # tables <- dbGetQuery(con, query)
  tables <- tbl(con, sql(query))
  
  return(tables)
}

column_dict_supplementary = c(
  "definition",
  "units",
  "accuracy",
  "scale",
  "format",
  "natural_key",
  "reviewed"
)


dict_supplementary_mutate <- function(dict, supplementary) {
  for (new_cols in supplementary) {
    dict[,new_cols] <- NA
  }
  
  dict$reviewed = FALSE
  
  return(dict)
}

tables_pkey = c("table_schema", "table_name")
columns_pkey = c("table_schema", "table_name", "column_name")

append_orphan_update_duplicate = function (dataframe_a, dataframe_b, key_columns){
  data_bind = bind_rows(dataframe_a %>%
                          mutate(d_source = "a"),
                        dataframe_b %>%
                          mutate(d_source = "b")
                        )
  
  data_pkey_counts = data_bind %>%
    group_by_at(key_columns) %>%
    count()
  
  # case 1: unique to data_frame_a -> append
  # logic: pkey combo found 1x in data-_bind, from dataframe_a
  data_append = data_pkey_counts %>%
    filter(n == 1) %>%
    select(-n) %>%
    left_join(data_bind, by=key_columns) %>%
    filter(d_source == "a") %>%
    select(-d_source)
  
  # case 2: unique to data_frame_b -> orphan
  # logic: pkey compo found 1x in data_bind, from dataframe_b
  data_orphan = data_pkey_counts %>%
    filter(n == 1) %>%
    select(-n) %>%
    left_join(data_bind, by=key_columns) %>%
    filter(d_source == "b") %>%
    select(-d_source)
  
  # case 3: non-identical rows sharing pkeys -> update (from data_frame_a)
  # logic: pkey combo found 2x in data_bind, is distinct among non-pkey columns
  data_update = data_pkey_counts %>%
    filter(n == 2) %>%
    select(-n) %>%
    left_join(data_bind, by=key_columns) %>%
    arrange(desc(d_source)) %>%
    distinct(pick(-d_source), .keep_all=TRUE) %>%
    filter(d_source == "a") %>%
    select(-d_source)
  
  # case 4: duplicate rows
  # logic: pkey combo found 2x in data_bind, 
  data_duplicate = dataframe_a %>%
      anti_join(data_update, by=key_columns) %>%
      anti_join(data_append, by=key_columns)
  
  return(list(data_append, data_orphan, data_update, data_duplicate))
}

```

## Pull Schemas

```{r}
schemas <- dbGetQuery(con, "SELECT schema_name FROM information_schema.schemata")$schema_name
schemas <- schemas[!schemas %in% c("information_schema", "pg_catalog")]
```

## Pull pg data and metadata tables

```{r}
# define lists
pg_table_data = list()
pg_column_data = list()

meta_table_data = list()
meta_column_data = list()

# columns derived from postegres information_schema
meta_column_data_pg = list()
# columns derived from manual human input
meta_column_data_manual = list()

for (schema in schemas){
  # pull pg data from information_schema
  pg_table_data[[schema]] = pull_pg_table_data(schema)
  pg_column_data[[schema]] = pull_pg_column_data(schema)
  
  # pull metadata tables from each schema
  # meta_table_data[[schema]] = dbReadTable(con, Id(schema, "metadata_tables"))
  # meta_column_data[[schema]] = dbReadTable(con, Id(schema, "metadata_columns"))
  meta_table_data[[schema]] = tbl(con, Id(schema, "metadata_tables"))
  meta_column_data[[schema]] = tbl(con, Id(schema, "metadata_columns"))
  
  meta_column_data_pg[[schema]] = meta_column_data[[schema]] %>%
    select(-any_of(column_dict_supplementary))
  meta_column_data_manual[[schema]] = meta_column_data[[schema]] %>%
    select(any_of(columns_pkey),
           any_of(column_dict_supplementary))
  
  cat("Data pulled for schema:", schema, "\n")
}

```

## Compare pg and metadata for diferences

-   check to make sure columns align between pg and metadata tables

```{r}
# QC
table_comp = list()
column_comp = list()
for (schema in schemas){
  if (!identical(colnames(meta_table_data[[schema]]), colnames(pg_table_data[[schema]]))) {
    stop(paste0("Columns in pg_table_data and meta_table_data do not align for schema '", schema))
  }
  
  if (!identical(colnames(meta_column_data_pg[[schema]]), colnames(pg_column_data[[schema]]))) {
    stop(paste0("Columns in pg_column_data and meta_column_common do not align for schema '", schema))
  }
}

```

## New rows to append to metadata

```{r}
meta_table_append = list()
meta_column_append = list()

for (schema in schemas){
  
  # query for new rows to append to metadata_tables (ie new tables within schema)
  meta_table_append[[schema]] = pg_table_data[[schema]] %>%
    anti_join(meta_table_data[[schema]], by=tables_pkey)
  
  # announce when new table data to append found
  if (meta_table_append[[schema]] %>% count() %>% pull(n) >0) {
    cat("New rows to append to metadata_tables in schema:", schema, "\n")
  }
  
  # query for new rows to append to metadata_columns (ie new columns within schema)
  meta_column_append[[schema]] = pg_column_data[[schema]] %>%
    anti_join(meta_column_data_pg[[schema]], by=columns_pkey)
  
  # announce when new column data to append found
  if (meta_column_append[[schema]] %>% count() %>% pull(n) >0) {
    cat("New rows to append to metadata_columns in schema:", schema, "\n")
  }
}


```

## Process discrepancies as: append, orphan, update, or duplicate

```{r}

meta_table_append = list()
meta_table_orphan = list()
meta_table_update = list()
meta_table_duplicate = list()

meta_column_append = list()
meta_column_orphan = list()
meta_column_update = list()
meta_column_duplicate = list()

for (schema in schemas) {
  table_results = append_orphan_update_duplicate(pg_table_data[[schema]] %>%
                                                   collect(),
                                                 meta_table_data[[schema]] %>%
                                                   collect(),
                                                 tables_pkey)
  meta_table_append[[schema]] = table_results[1]
  meta_table_orphan[[schema]] = table_results[2]
  meta_table_update[[schema]] = table_results[3]
  meta_table_duplicate[[schema]] = table_results[4]
  
  column_results = append_orphan_update_duplicate(pg_column_data[[schema]] %>%
                                                    collect(),
                                                  meta_column_data_pg[[schema]] %>%
                                                    collect(),
                                                  columns_pkey)
  meta_column_append[[schema]] = column_results[1]
  meta_column_orphan[[schema]] = column_results[2]
  meta_column_update[[schema]] = column_results[3]
  meta_column_duplicate[[schema]] = column_results[4]

  cat("Discrepancies processed for schema: ", schema, "\n")
}


```

## warnings for orphan data

```{r}
for (schema in schemas) {
  if (nrow(meta_table_orphan[[schema]][[1]]) > 0) {
    warning(paste0("Orphan table data found in schema:", schema))
  }
  
  if (nrow(meta_table_orphan[[schema]][[1]]) > 0) {
    warning(paste0("Orphan column data found in schema:", schema))
  }
}
```

## append and update database using single transaction

```{r}

dbBegin(con)

tryCatch(
  {
    # appends
    for (schema in schemas) {
      if (nrow(meta_table_append[[schema]][[1]]) > 0) {
        dbWriteTable(con,
                     Id(schema, "metadata_tables"),
                     meta_table_append[[schema]][[1]],
                     append = TRUE,
                     row.names = FALSE)
        
        cat("generating SQL to append to", schema,".metadata_tables\n")
      }
      
      if (nrow(meta_column_append[[schema]][[1]]) > 0) {
        dbWriteTable(con,
                     Id(schema, "metadata_columns"),
                     meta_column_append[[schema]][[1]],
                     append = TRUE,
                     row.names = FALSE)
        
        cat("generating SQL to append to", schema,".metadata_columns\n")
      }
      
      # updates
      if (nrow(meta_table_update[[schema]][[1]]) > 0) {
        rows_update(meta_table_data[[schema]],
                    meta_table_update[[schema]][[1]],
                    by=tables_pkey)
        
        cat("generating SQL to update ", schema,".metadata_tables\n")
        }
        
      if (nrow(meta_column_update[[schema]][[1]]) > 0) {
        rows_update(meta_column_data[[schema]],
                    meta_column_update[[schema]][[1]],
                    by=columns_pkey)
        
        cat("generating SQL to update ", schema,".metadata_columns\n")
      }
    }
  
  
  # Commit the transaction if successful
  dbCommit(con)
  cat("Transaction successful!")
  
}, error = function(e) {
  # Rollback in case of error
  dbRollback(con)
  message("Transaction failed: ", e$message)
})
```

# Write metadata tables to csv locally

```{r}

for (schema in schemas) {
  dir.create(file.path(here("staging", "metadata", schema)), showWarnings = FALSE)

  write.csv(meta_table_data[[schema]], here("staging", "metadata", schema, "table_dict.csv"), row.names = FALSE)
  write.csv(meta_column_data[[schema]], here("staging", "metadata", schema, "column_dict.csv"), row.names = FALSE)
  
  cat(".csv tables written for schema: ", schema, "\n")
}

write.csv(schemas, here("staging", "metadata","schemas.csv"), row.names = FALSE)

```

# Read files in from csv locally

```{r}

meta_table_csv = list()
meta_column_csv = list()

char_cols = column_dict_supplementary[! column_dict_supplementary %in% c("natural_key", "reviewed")]

for (schema in schemas) {
  # Load dictionaries from file
  meta_table_csv[[schema]] = read.csv(here("staging", "metadata_revision", schema, "table_dict.csv"))
  meta_column_csv[[schema]] = read.csv(here("staging", "metadata_revision", schema, "column_dict.csv")) %>%
    mutate_at(char_cols, as.character)
}

```

## look for discrepancies in manual info only

```{r}
meta_column_csv_manual = list()

meta_column_manual_append = list()
meta_column_manual_orphan = list()
meta_column_manual_update = list()
meta_column_manual_duplicate = list()

for (schema in schemas) {
  meta_column_csv_manual[[schema]] = meta_column_csv[[schema]] %>%
    select(any_of(columns_pkey),
           any_of(column_dict_supplementary))
  
  results = append_orphan_update_duplicate(meta_column_csv_manual[[schema]],
                                 meta_column_data_manual[[schema]] %>% collect(),
                                 columns_pkey)
  
  meta_column_manual_append[[schema]] = results[1]
  meta_column_manual_orphan[[schema]] = results[2]
  meta_column_manual_update[[schema]] = results[3]
  meta_column_manual_duplicate[[schema]] = results[4]
  
  cat("Discrepancies processed for schema: ", schema, "\n")
}




```

## Define function to maintain & update dictionaries

```{r}

# write tables

pull_dictionaries <- function(schemas) {
  for (schema in schemas) {
    table_dict_pull <- dbReadTable(con, Id(schema, paste0("metadata_tables")))
    column_dict_pull <- dbReadTable(con, Id(schema, paste0("metadata_columns")))
    
    dir.create(file.path(here("staging", "metadata", schema)), showWarnings = FALSE)
  
    write.csv(column_dict_pull, here("staging", "metadata", schema, "column_dict.csv"), row.names = FALSE)
    write.csv(table_dict_pull, here("staging", "metadata", schema, "table_dict.csv"), row.names = FALSE)
  }
}


```

```{r}
compare_dictionaries <- function(schema) {
  # Get current column dictionary
  column_dict_read <- dbReadTable(con, Id(schema, paste0(schema, "_column_dictionary"))) %>%
    select(-column_dict_supplementary)  #Troubleshoot this!
  
  # Get current table dictionary
  table_dict_read <- dbReadTable(con, Id(schema, paste0(schema, "_table_dictionary")))
  
  # Build new dictionaries
  new_column_dict <- build_column_dictionary(schema)
  new_table_dict <- build_table_dictionary(schema)
  
  # Check for changes in columns
  column_changes <- anti_join(new_column_dict, current_column_dict, 
                              by = c("table_schema", "table_name", "column_name"))
  
  # Check for changes in tables
  table_changes <- anti_join(new_table_dict, current_table_dict, 
                             by = c("table_schema","table_name"))
  
  if (nrow(column_changes) > 0 || nrow(table_changes) > 0) {
    cat("Changes detected in schema:", schema, "\n")
    
    if (nrow(column_changes) > 0) {
      cat("Column changes:\n")
      print(column_changes)
    }
    
    if (nrow(table_changes) > 0) {
      cat("Table changes:\n")
      print(table_changes)
    }
    
    update <- readline(prompt = "Do you want to update the dictionaries? (y/n): ")
    
    if (tolower(update) == "y") {
      # Update column dictionary
      dbWriteTable(con, 
                   name = c(schema, paste0(schema, "_column_dictionary")),
                   value = new_column_dict, 
                   overwrite = TRUE)
      
      # Update table dictionary
      dbWriteTable(con, 
                   name = c(schema, paste0(schema, "_table_dictionary")),
                   value = new_table_dict, 
                 overwrite = TRUE)
      
      cat("Dictionaries updated for schema:", schema, "\n")
    } else {
      cat("Update cancelled for schema:", schema, "\n")
    }
  } else {
    cat("No changes detected in schema:", schema, "\n")
  }
}
```

## Update dictionaries

```{r}
for (schema in schemas) {
  update_dictionaries(schema)
}
```

## Close connection
