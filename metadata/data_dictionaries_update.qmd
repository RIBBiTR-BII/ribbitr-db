---
title: "Update Data Dictionaries"
author: "Cob Staines"
format: html
editor: source
---

```{r setup, include=FALSE}
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

# librarian downloads, if not already downloaded, and reads in needed packages
librarian::shelf(tidyverse, DBI, RPostgres, here, janitor)
```

## Connect to DB

```{r}
# establish connection
tryCatch({
  print("Connecting to Databaseâ€¦")
  con <- dbConnect(dbDriver("Postgres"),
                          dbname = Sys.getenv("aws_dbname"),
                          host = Sys.getenv("aws_host"),
                          port = Sys.getenv("aws_port"),
                          user = Sys.getenv("aws_user"),
                          password = Sys.getenv("aws_password"),
                          timezone=NULL)
  print("Database Connected!")
},
error=function(cond) {
  message("Unable to connect to Database: ", cond$message)
})

```

## Define functions to pull table and column metadata

```{r}
pull_pg_column_data <- function(schema) {
  query <- paste0("
    SELECT 
      c.table_schema,
      c.table_name,
      c.column_name,
      c.data_type,
      c.character_maximum_length,
      c.numeric_precision,
      c.datetime_precision,
      c.is_nullable,
      c.column_default,
      c.ordinal_position,
      pg_catalog.col_description(format('%s.%s',c.table_schema,c.table_name)::regclass::oid, c.ordinal_position) as pg_description,
      CASE 
        WHEN tc.constraint_type = 'PRIMARY KEY' THEN 'PK'
        WHEN tc.constraint_type = 'FOREIGN KEY' THEN 'FK'
        WHEN tc.constraint_type = 'UNIQUE' THEN 'UQ'
        ELSE NULL
      END as key_type
    FROM 
      information_schema.columns c
    LEFT JOIN 
      information_schema.key_column_usage kcu
      ON c.table_schema = kcu.table_schema
      AND c.table_name = kcu.table_name
      AND c.column_name = kcu.column_name
    LEFT JOIN 
      information_schema.table_constraints tc
      ON kcu.table_schema = tc.table_schema
      AND kcu.table_name = tc.table_name
      AND kcu.constraint_name = tc.constraint_name
    WHERE 
      c.table_schema = '", schema, "'
    ORDER BY 
      c.table_name, c.ordinal_position
  ")
  
  # columns <- dbGetQuery(con, query)
  columns <- tbl(con, sql(query))
  
  return(columns)
}

pull_pg_table_data <- function(schema) {
  query <- paste0("
    SELECT 
      t.table_schema, t.table_name,
      (SELECT count(*) FROM information_schema.columns c WHERE c.table_name = t.table_name AND c.table_schema = t.table_schema) as column_count,
      pg_catalog.obj_description(format('%s.%s',t.table_schema,t.table_name)::regclass::oid, 'pg_class') as table_description
    FROM 
      information_schema.tables t
    WHERE 
      t.table_schema = '", schema, "'
  ")
  
  # tables <- dbGetQuery(con, query)
  tables <- tbl(con, sql(query))
  
  return(tables)
}

column_dict_supplementary = c(
  "definition",
  "units",
  "accuracy",
  "scale",
  "format",
  "natural_key",
  "reviewed"
)


dict_supplementary_mutate <- function(dict, supplementary) {
  for (new_cols in supplementary) {
    dict[,new_cols] <- NA
  }
  
  dict$reviewed = FALSE
  
  return(dict)
}

tables_pkey = c("table_schema", "table_name")
columns_pkey = c("table_schema", "table_name", "column_name")

append_orphan_update_duplicate = function (dataframe_a, dataframe_b, key_columns){
  data_bind = bind_rows(dataframe_a %>%
                          mutate(d_source = "a"),
                        dataframe_b %>%
                          mutate(d_source = "b")
                        )
  
  data_pkey_counts = data_bind %>%
    group_by_at(key_columns) %>%
    count() %>%
    ungroup()
  
  # case 1: unique to data_frame_a -> append
  # logic: pkey combo found 1x in data-_bind, from dataframe_a
  data_append = data_pkey_counts %>%
    filter(n == 1) %>%
    select(-n) %>%
    left_join(data_bind, by=key_columns) %>%
    filter(d_source == "a") %>%
    select(-d_source)
  
  # case 2: unique to data_frame_b -> orphan
  # logic: pkey compo found 1x in data_bind, from dataframe_b
  data_orphan = data_pkey_counts %>%
    filter(n == 1) %>%
    select(-n) %>%
    left_join(data_bind, by=key_columns) %>%
    filter(d_source == "b") %>%
    select(-d_source)
  
  # case 3: non-identical rows sharing pkeys -> update (from data_frame_a)
  # logic: pkey combo found 2x in data_bind, is distinct among non-pkey columns
  data_update = data_pkey_counts %>%
    filter(n == 2) %>%
    select(-n) %>%
    left_join(data_bind, by=key_columns) %>%
    arrange(desc(d_source)) %>%
    distinct_at(vars(-d_source), .keep_all=TRUE) %>%
    filter(d_source == "a") %>%
    select(-d_source)
  
  # case 4: duplicate rows
  # logic: pkey combo found 2x in data_bind, 
  data_duplicate = dataframe_a %>%
      anti_join(data_update, by=key_columns) %>%
      anti_join(data_append, by=key_columns)
  
  return(list(data_append, data_orphan, data_update, data_duplicate))
}

```

## Pull Schemas

```{r}
schemas <- dbGetQuery(con, "SELECT schema_name FROM information_schema.schemata
                      WHERE schema_name NOT LIKE 'pg_temp_%'
                      AND schema_name NOT LIKE 'pg_toast_temp_%'
                      AND schema_name != 'pg_catalog'
                      AND schema_name != 'information_schema';")$schema_name
```

## Pull pg data and metadata tables

```{r}
# define lists
pg_table_data = list()
pg_column_data = list()

meta_table_data = list()
meta_column_data = list()

# columns derived from postgres information_schema
meta_column_data_pg = list()
# supplementary columns derived from manual human input
meta_column_data_sup = list()

for (schema in schemas){
  # pull pg data from information_schema
  pg_table_data[[schema]] = pull_pg_table_data(schema)
  pg_column_data[[schema]] = pull_pg_column_data(schema)
  
  # pull metadata tables from each schema, dropping supplementary columns
  meta_table_data[[schema]] = tbl(con, Id(schema, "metadata_tables"))
  meta_column_data[[schema]] = tbl(con, Id(schema, "metadata_columns"))
  
  meta_column_data_pg[[schema]] = meta_column_data[[schema]] %>%
    select(-any_of(column_dict_supplementary))
  meta_column_data_sup[[schema]] = meta_column_data[[schema]] %>%
    select(any_of(columns_pkey),
           any_of(column_dict_supplementary))
  
  cat("Data pulled for schema:", schema, "\n")
}

```

## QA: Compare pg and metadata for diferences

-   check to make sure columns align between pg and metadata tables

```{r}
# QC
table_comp = list()
column_comp = list()
for (schema in schemas){
  if (!identical(colnames(meta_table_data[[schema]]), colnames(pg_table_data[[schema]]))) {
    stop(paste0("Columns in pg_table_data and meta_table_data do not align for schema '", schema))
  }
  
  if (!identical(colnames(meta_column_data_pg[[schema]]), colnames(pg_column_data[[schema]]))) {
    stop(paste0("Columns in pg_column_data and meta_column_common do not align for schema '", schema))
  }
}

```

## Process discrepancies as: append, orphan, update, or duplicate

```{r}

meta_table_append = list()
meta_table_orphan = list()
meta_table_update = list()
meta_table_duplicate = list()

meta_column_append = list()
meta_column_orphan = list()
meta_column_update = list()
meta_column_duplicate = list()

for (schema in schemas) {
  table_results = append_orphan_update_duplicate(pg_table_data[[schema]] %>%
                                                   collect(),
                                                 meta_table_data[[schema]] %>%
                                                   collect(),
                                                 tables_pkey)
  meta_table_append[[schema]] = table_results[1][[1]]
  meta_table_orphan[[schema]] = table_results[2][[1]]
  meta_table_update[[schema]] = table_results[3][[1]]
  meta_table_duplicate[[schema]] = table_results[4][[1]]
  
  column_results = append_orphan_update_duplicate(pg_column_data[[schema]] %>%
                                                    collect(),
                                                  meta_column_data_pg[[schema]] %>%
                                                    collect(),
                                                  columns_pkey)
  meta_column_append[[schema]] = column_results[1][[1]]
  meta_column_orphan[[schema]] = column_results[2][[1]]
  meta_column_update[[schema]] = column_results[3][[1]]
  meta_column_duplicate[[schema]] = column_results[4][[1]]

  cat("Discrepancies processed for schema: ", schema, "\n")
}


```

## warnings for orphan data, flag for update or append

```{r}
update = FALSE
append = FALSE

for (schema in schemas) {
  # warnings for orphan data
  if (nrow(meta_table_orphan[[schema]]) > 0) {
    warning(paste0("Table orphan data found in schema:", schema))
  }
  if (nrow(meta_column_orphan[[schema]]) > 0) {
    warning(paste0("Column orphan data found in schema:", schema))
  }
  
  # flag if updates needed
  if (nrow(meta_table_update[[schema]]) > 0) {
    update = TRUE
    cat("Table updates found for schema: ", schema, "\n")
  }
  if (nrow(meta_column_update[[schema]]) > 0) {
    update = TRUE
    cat("Column updates found for schema: ", schema, "\n")
  }
  
  # flag if append needed
  if (nrow(meta_table_append[[schema]]) > 0) {
    append = TRUE
    cat("Table appends found for schema: ", schema, "\n")
  }
  if (nrow(meta_column_append[[schema]]) > 0) {
    append = TRUE
    cat("Column appends found for schema: ", schema, "\n")
  }
}
```

## append and update database using single transaction

```{r}

generate_update_sql <- function(table_name, key_columns, updates) {
  # Validate inputs
  if (length(key_columns) == 0) {
    stop("Key columns must not be empty")
  }
  if (nrow(updates) == 0) {
    stop("updates must have at least one row.")
  }
  
  # Initialize an empty list to store SQL statements
  sql_statements <- vector("list", nrow(updates))
  
  # Loop through each row of the updates data frame
  for (ii in seq_len(nrow(updates))) {
    row <- updates[ii, ]
    
    # Construct the SET clause
    set_clause <- paste(
      sapply(names(row), function(col) {
        if (!(col %in% key_columns)) { # Exclude key columns from SET clause
          paste0(col, " = '", row[[col]], "'")
        }
      }), collapse = ", "
    )
    
    # Construct the WHERE clause
    where_clause <- paste(
      sapply(key_columns, function(col) {
        paste0(col, " = '", row[[col]], "'")
      }), collapse = " AND "
    )
    
    # Generate the final SQL statement
    sql <- paste0("UPDATE ", table_name, " SET ", set_clause, " WHERE ", where_clause, ";")
    
    # Store the SQL statement
    sql_statements[[ii]] <- sql
  }
  
  return(sql_statements)
}

# Example usage
table_name <- "metadata_columns"
key_columns <- columns_pkey
updates <- data.frame(
  id = c(1, 2),
  name = c("John Doe", "Jane Smith"),
  salary = c(75000, 80000)
)

# Generate SQL statements
sql_statements <- generate_update_sql("metadata_columns", columns_pkey, meta_column_update[[schema]])

# Print SQL statements
cat(paste(sql_statements, collapse = "\n"))

if (append | update){
  
  dbBegin(con)
  
  tryCatch(
    {
      # appends
      for (schema in schemas) {
         if (append){
          # append table
          if (nrow(meta_table_append[[schema]]) > 0) {
            dbWriteTable(con,
                         Id(schema, "metadata_tables"),
                         meta_table_append[[schema]],
                         append = TRUE,
                         row.names = FALSE)
            
            cat("Appended ", schema,".metadata_tables\n")
          }
          
          # append column
          if (nrow(meta_column_append[[schema]]) > 0) {
            dbWriteTable(con,
                         Id(schema, "metadata_columns"),
                         meta_column_append[[schema]],
                         append = TRUE,
                         row.names = FALSE)
            
            cat("Appended ", schema,".metadata_columns\n")
          }
        }
        
        # updates
        if (update) {
          # update table
          if (nrow(meta_table_update[[schema]]) > 0) {
            meta_table_data_update = meta_table_data[[schema]] %>%
              filter(FALSE) %>%
              collect() %>%
              bind_rows(meta_table_update[[schema]])
            
            rows_update(meta_table_data[[schema]],
                        meta_table_data_update,
                        by=tables_pkey)
            
            cat("Updated ", schema,".metadata_tables\n")
            }
          
          # book mark! Errors everywhere using rows_update()
          
          # update column
          if (nrow(meta_column_update[[schema]]) > 0) {
            # meta_column_data_update = meta_column_data[[schema]] %>%
            #   filter(FALSE) %>%
            #   collect() %>%
            #   bind_rows(meta_column_update[[schema]])
            meta_column_data_update = meta_column_data[[schema]] %>%
              select(all_of(columns_pkey)) %>%
              right_join(meta_column_update[[schema]])
            
            update_query = meta_column_data[[schema]] %>%
              rows_update(
                meta_column_update[[schema]],
                by=columns_pkey,
                copy = TRUE,
                unmatched = "ignore")
            
            query_sql = dbplyr::sql_render(update_query)
            
            dbExecute(con, query_sql)
            
            meta_column_data[[schema]] %>%
              filter(table_name == "metadata_columns") %>%
              select(-any_of(column_dict_supplementary)) %>%
              arrange(column_name)
            meta_column_update[[schema]] %>%
              arrange(column_name)
            
            cat("Updated ", schema,".metadata_columns\n")
          }
        }
      }
    
    # Commit the transaction if successful
    dbCommit(con)
    cat("Transaction successful! Metadata is up to date!\n")
    
  }, error = function(e) {
    # Rollback in case of error
    dbRollback(con)
    message("Transaction failed: ", e$message)
  })
} else {
  cat("No updates or appends found. Metadata is up to date!\n")
}
```

# Write metadata tables to csv locally

```{r}

for (schema in schemas) {
  dir.create(file.path(here("staging", "metadata", schema)), showWarnings = FALSE)

  write.csv(meta_table_data[[schema]], here("staging", "metadata", schema, "table_dict.csv"), row.names = FALSE)
  write.csv(meta_column_data[[schema]], here("staging", "metadata", schema, "column_dict.csv"), row.names = FALSE)
  
  cat(".csv tables written for schema: ", schema, "\n")
}

write.csv(schemas, here("staging", "metadata","schemas.csv"), row.names = FALSE)

```

# Read files in from csv locally

```{r}

meta_table_csv = list()
meta_column_csv = list()

char_cols = column_dict_supplementary[! column_dict_supplementary %in% c("natural_key", "reviewed")]

for (schema in schemas) {
  # Load dictionaries from file
  meta_table_csv[[schema]] = read.csv(here("staging", "metadata_revision", schema, "table_dict.csv"))
  meta_column_csv[[schema]] = read.csv(here("staging", "metadata_revision", schema, "column_dict.csv")) %>%
    mutate_at(char_cols, as.character) %>%
    mutate(natural_key = as.logical(natural_key),
           reviewed = as.logical(reviewed))
  meta_column_csv[[schema]]$natural_key[is.na(meta_column_csv[[schema]]$natural_key)] = FALSE  #remove after first pass
  
}

```

## look for discrepancies in supplementary (manual) columns only

```{r}
meta_column_csv_manual = list()

meta_column_manual_append = list()
meta_column_manual_orphan = list()
meta_column_manual_update = list()
meta_column_manual_duplicate = list()

for (schema in schemas) {
  meta_column_csv_manual[[schema]] = meta_column_csv[[schema]] %>%
    select(any_of(columns_pkey),
           any_of(column_dict_supplementary))
  
  results = append_orphan_update_duplicate(meta_column_csv_manual[[schema]],
                                 meta_column_data_manual[[schema]] %>% collect(),
                                 columns_pkey)
  
  meta_column_manual_append[[schema]] = results[1][[1]]
  meta_column_manual_orphan[[schema]] = results[2][[1]]
  # meta_column_manual_update[[schema]] = results[3][[1]]
  meta_column_manual_update[[schema]] = meta_column_data_manual[[schema]] %>%
    select(columns_pkey) %>%
    right_join(results[3][[1]], by=columns_pkey, copy=TRUE) # super slow, find workaround
  meta_column_manual_duplicate[[schema]] = results[4][[1]]
  
  cat("Discrepancies processed for schema: ", schema, "\n")
}

```
## append and update database using single transaction

```{r}

dbBegin(con)

tryCatch(
  {
    # appends
    for (schema in schemas) {
        
      if (any(is.na(meta_column_data_manual[[schema]] %>% select(columns_pkey)))) {
          stop("NA values found in the primary key of meta_column_data_manual.")
      }
      if (any(is.na(meta_column_data_manual[[schema]] %>% select(columns_pkey)))) {
          stop("NA values found in the primary key of meta_column_manual_update.")
      }
      
      cat("Data to be updated:\n")
      print(meta_column_manual_update[[schema]] %>% collect())
      cat("Existing data:\n")
      print(meta_column_data_manual[[schema]] %>% collect())
      
      if (nrow(meta_column_manual_update[[schema]]) > 0) {
        query <- rows_update(meta_column_data_manual[[schema]],  # many issues with this function, so far not successful. Workaround: Generate explicit SQL to drop rows to update, then append updates. 
                    meta_column_manual_update[[schema]],
                    by=columns_pkey)
        print(show_query(query))
        cat("Updated ", schema,".metadata_columns\n")
      }
    }
  
  
  # Commit the transaction if successful
  dbCommit(con)
  cat("Transaction successful!")
  
}, error = function(e) {
  # Rollback in case of error
  dbRollback(con)
  message("Transaction failed: ", e$message)
})
```