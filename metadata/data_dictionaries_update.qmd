---
title: "Update Data Dictionaries"
author: "Cob Staines"
format: html
editor: source
---

```{r setup, include=FALSE}
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

# librarian downloads, if not already downloaded, and reads in needed packages
librarian::shelf(tidyverse, DBI, RPostgres, here, janitor)

## Connect to DB

# establish connection
tryCatch({
  print("Connecting to Databaseâ€¦")
  con <- dbConnect(dbDriver("Postgres"),
                          dbname = Sys.getenv("aws_dbname"),
                          host = Sys.getenv("aws_host"),
                          port = Sys.getenv("aws_port"),
                          user = Sys.getenv("aws_user"),
                          password = Sys.getenv("aws_password"),
                          timezone=NULL)
  print("Database Connected!")
},
error=function(cond) {
  message("Unable to connect to Database: ", cond$message)
})

```

## Define functions to pull table and column metadata

```{r}
pull_pg_column_data <- function(schema) {
  query <- paste0("
    SELECT 
      c.table_schema,
      c.table_name,
      c.column_name,
      c.data_type,
      c.character_maximum_length,
      c.numeric_precision,
      c.datetime_precision,
      c.is_nullable,
      c.column_default,
      c.ordinal_position,
      pg_catalog.col_description(format('%s.%s',c.table_schema,c.table_name)::regclass::oid, c.ordinal_position) as pg_description,
      CASE 
        WHEN tc.constraint_type = 'PRIMARY KEY' THEN 'PK'
        WHEN tc.constraint_type = 'FOREIGN KEY' THEN 'FK'
        WHEN tc.constraint_type = 'UNIQUE' THEN 'UQ'
        ELSE NULL
      END as key_type
    FROM 
      information_schema.columns c
    LEFT JOIN 
      information_schema.key_column_usage kcu
      ON c.table_schema = kcu.table_schema
      AND c.table_name = kcu.table_name
      AND c.column_name = kcu.column_name
    LEFT JOIN 
      information_schema.table_constraints tc
      ON kcu.table_schema = tc.table_schema
      AND kcu.table_name = tc.table_name
      AND kcu.constraint_name = tc.constraint_name
    WHERE 
      c.table_schema = '", schema, "'
    ORDER BY 
      c.table_name, c.ordinal_position
  ")
  
  # columns <- dbGetQuery(con, query)
  columns <- tbl(con, sql(query))
  
  return(columns)
}

pull_pg_table_data <- function(schema) {
  query <- paste0("
    SELECT 
      t.table_schema, t.table_name,
      (SELECT count(*) FROM information_schema.columns c WHERE c.table_name = t.table_name AND c.table_schema = t.table_schema) as column_count,
      pg_catalog.obj_description(format('%s.%s',t.table_schema,t.table_name)::regclass::oid, 'pg_class') as table_description
    FROM 
      information_schema.tables t
    WHERE 
      t.table_schema = '", schema, "'
  ")
  
  # tables <- dbGetQuery(con, query)
  tables <- tbl(con, sql(query))
  
  return(tables)
}

column_dict_supplementary = c(
  "definition",
  "units",
  "accuracy",
  "scale",
  "format",
  "natural_key",
  "reviewed"
)


dict_supplementary_mutate <- function(dict, supplementary) {
  for (new_cols in supplementary) {
    dict[,new_cols] <- NA
  }
  
  dict$reviewed = FALSE
  
  return(dict)
}

tables_pkey = c("table_schema", "table_name")
columns_pkey = c("table_schema", "table_name", "column_name")

# dataframe_a = pg_column_data[[schema]] %>% collect()
# dataframe_b = meta_column_data_pg[[schema]] %>% collect()
# key_columns = columns_pkey
# return_all = TRUE 

compare_for_staging = function(dataframe_a, dataframe_b, key_columns, insert=TRUE, update=TRUE, orphan=FALSE, duplicate=FALSE, return_all=FALSE){
  # dataframe_a is NEW data
  # dataframe_b is OLD data
  
  output = list()
  
  # case 1: identical row appears in both dataframes
  # logic: find duplicate rows
  data_bind = bind_rows(dataframe_a, dataframe_b)
  data_duplicate = data_bind[duplicated(data_bind),]
  if (duplicate || return_all){
    output[["duplicate"]] = data_duplicate
  }
  
  # bind rows ignoring duplicates, tracking source
  data_bind_uni = data_bind %>%
    anti_join(data_duplicate, by = key_columns)
  
  # count grouped by key_columns (1 or 2)
  data_pkey_counts = data_bind_uni %>%
    group_by_at(key_columns) %>%
    count() %>%
    ungroup()
  
  data_bind_source = bind_rows(dataframe_a %>%
                        mutate(d_source = "a"),
                      dataframe_b %>%
                        mutate(d_source = "b"))
  
  # case 2: unique to data_frame_b -> orphan
  # logic: pkey compo found 1x in data_bind, from dataframe_b
  if (orphan || return_all){
    data_orphan = data_pkey_counts %>%
      filter(n == 1) %>%
      select(-n) %>%
      inner_join(dataframe_b, by=key_columns)
      
    output[["orphan"]] = data_orphan
  }

  
  # case 3: unique to data_frame_a -> insert
  # logic: pkey combo found 1x in data_bind, from dataframe_a
  if (insert || return_all){
    data_insert = data_pkey_counts %>%
      filter(n == 1) %>%
      select(-n) %>%
      inner_join(dataframe_a, by=key_columns)
    
    output[["insert"]] = data_insert
  }

  
  # case 4: non-identical rows sharing pkeys -> update (from data_frame_a)
  # logic: pkey combo found 2x in data_bind, is distinct among non-pkey columns
  if (update || return_all){
    data_update = data_pkey_counts %>%
      filter(n == 2) %>%
      select(-n) %>%
      inner_join(dataframe_a, by=key_columns)
    
    output[["update"]] = data_update
  }

  
  return(output)
}


stage_to_temp <- function(con, reference_table, novel_data) {
  # check that all novel_data columns exist in reference table
  ref_cols = colnames(reference_table)
  nov_cols = colnames(novel_data)
  if (length(setdiff(nov_cols, ref_cols)) > 0) {
    stop("Columns in preference_table and novel_data do not align.")
  }
  
  # build meaningful temp table name: [schema].[reference_table]_temp
  table_path = as.character(reference_table$lazy_query$x)
  path_parts = strsplit(gsub("\"", "", table_path), "\\.")
  schema_name = path_parts[[1]][1]
  table_name = path_parts[[1]][2]
  temp_table_name = paste0(schema_name, "_", table_name, "_temp")
  
  # begin transaction

  # drop table if exists
  suppressMessages(
    dbExecute(con, paste0("DROP TABLE IF EXISTS ", temp_table_name, ";"))
  )
  # copy reference table to temporary table
  dbExecute(con, paste0("CREATE TEMP TABLE ", temp_table_name, " AS SELECT * FROM ", schema_name, ".", table_name, ";"))
  # drop all existing rows
  dbExecute(con, paste0("TRUNCATE TABLE ", temp_table_name))
  # drop all columns in reference_table not in novel_data
  drop_cols = setdiff(ref_cols, nov_cols)
  if (length(drop_cols) > 0){
    dbExecute(con, paste0("ALTER TABLE ", temp_table_name, " DROP COLUMN ", paste(drop_cols, collapse = ", DROP COLUMN ")))
  }
  # write all novel data to temp table
  dbWriteTable(con, name = temp_table_name, value = novel_data, append = TRUE)

  
  return(temp_table_name)
}

```

## Pull Schemas

```{r}
schemas <- dbGetQuery(con, "SELECT schema_name FROM information_schema.schemata
                      WHERE schema_name NOT LIKE 'pg_temp_%'
                      AND schema_name NOT LIKE 'pg_toast_temp_%'
                      AND schema_name != 'pg_catalog'
                      AND schema_name != 'information_schema'
                      AND schema_name != 'public';")$schema_name
```

## Pull pg data and metadata tables

```{r}
# define lists
pg_table_data = list()
pg_column_data = list()

meta_table_data = list()
meta_column_data = list()

# columns derived from postgres information_schema
meta_column_data_pg = list()
# supplementary columns derived from manual human input
meta_column_data_sup = list()

for (schema in schemas){
  # pull pg data from information_schema
  pg_table_data[[schema]] = pull_pg_table_data(schema)
  pg_column_data[[schema]] = pull_pg_column_data(schema)
  
  # pull metadata tables from each schema, dropping supplementary columns
  meta_table_data[[schema]] = tbl(con, Id(schema, "metadata_tables"))
  meta_column_data[[schema]] = tbl(con, Id(schema, "metadata_columns"))
  
  meta_column_data_pg[[schema]] = meta_column_data[[schema]] %>%
    select(-any_of(column_dict_supplementary))
  meta_column_data_sup[[schema]] = meta_column_data[[schema]] %>%
    select(any_of(columns_pkey),
           any_of(column_dict_supplementary))
  
  cat("Data pulled for schema:", schema, "\n")
}

```

## QA: Compare pg and metadata for diferences

-   check to make sure columns align between pg and metadata tables

```{r}
# QC
table_comp = list()
column_comp = list()
for (schema in schemas){
  if (!identical(colnames(meta_table_data[[schema]]), colnames(pg_table_data[[schema]]))) {
    stop(paste0("Columns in pg_table_data and meta_table_data do not align for schema '", schema))
  }
  
  if (!identical(colnames(meta_column_data_pg[[schema]]), colnames(pg_column_data[[schema]]))) {
    stop(paste0("Columns in pg_column_data and meta_column_common do not align for schema '", schema))
  }
}

```

## Compare for staging

```{r}

meta_table_insert = list()
meta_table_orphan = list()
meta_table_update = list()
meta_table_duplicate = list()

meta_column_insert = list()
meta_column_orphan = list()
meta_column_update = list()
meta_column_duplicate = list()

for (schema in schemas) {
  table_results = compare_for_staging(pg_table_data[[schema]] %>%
                                       collect(),
                                     meta_table_data[[schema]] %>%
                                       collect(),
                                     tables_pkey,
                                     return_all= TRUE)
  meta_table_insert[[schema]] = table_results$insert
  meta_table_orphan[[schema]] = table_results$orphan
  meta_table_update[[schema]] = table_results$update
  meta_table_duplicate[[schema]] = table_results$duplicate
  
  column_results = compare_for_staging(pg_column_data[[schema]] %>%
                                         collect(),
                                       meta_column_data_pg[[schema]] %>%
                                         collect(),
                                       columns_pkey,
                                       return_all=TRUE)
  meta_column_insert[[schema]] = column_results$insert
  meta_column_orphan[[schema]] = column_results$orphan
  meta_column_update[[schema]] = column_results$update
  meta_column_duplicate[[schema]] = column_results$duplicate
  
  cat("Discrepancies processed for schema: ", schema, "\n")
}


```
## warnings for orphan data, flag for update or append

```{r}
update = FALSE
insert = FALSE

meta_table_upsert = list()
meta_column_upsert = list()

for (schema in schemas) {
  # warnings for orphan data
  if (nrow(meta_table_orphan[[schema]]) > 0) {
    warning(paste0("Table orphan data found in schema: ", schema, ".\n\t Investigate 'meta_table_orphan$", schema, "' and resolve manually.\n"))
  }
  if (nrow(meta_column_orphan[[schema]]) > 0) {
    warning(paste0("Column orphan data found in schema: ", schema, ".\n\t Investigate 'meta_column_orphan$", schema, "' and resolve manually.\n"))
  }
  
  # flag if append needed
  if (nrow(meta_table_insert[[schema]]) > 0) {
    insert = TRUE
    cat("Table inserts found for schema: ", schema, "\n")
  }
  if (nrow(meta_column_insert[[schema]]) > 0) {
    insert = TRUE
    cat("Column inserts found for schema: ", schema, "\n")
  }
  
  # flag if updates needed
  if (nrow(meta_table_update[[schema]]) > 0) {
    update = TRUE
    cat("Table updates found for schema: ", schema, "\n")
  }
  if (nrow(meta_column_update[[schema]]) > 0) {
    update = TRUE
    cat("Column updates found for schema: ", schema, "\n")
  }
  
  meta_table_upsert[[schema]] = rbind(meta_table_update[[schema]],
                                      meta_table_insert[[schema]])
  meta_column_upsert[[schema]] = rbind(meta_column_update[[schema]],
                                      meta_column_insert[[schema]])
  
}

cat("-------------\nUpdate: ", update, "\nInsert: ", insert, "\n")
```

## create temporary DB tables with upsert data, and upsert
```{r}
if (update || insert) {
  # begin transaction
  dbBegin(con)
  
  tryCatch(
    {
      for (schema in schemas) {
        # some local test to avoid below if not needed
        if (nrow(meta_table_upsert[[schema]]) > 0) {
          # upsert table_data
          reference_table = meta_table_data[[schema]]
          # novel_data = pg_table_data[[schema]] %>% collect()
          novel_data = meta_table_upsert[[schema]]
          temp_table_name = stage_to_temp(con, reference_table, novel_data)
          pointer = tbl(con, temp_table_name)
          rows_upsert(meta_table_data[[schema]], pointer, by=tables_pkey, in_place=TRUE)
          
          cat("Tables upserted for schema: ", schema, "\n")
        }
        
        if (nrow(meta_column_upsert[[schema]]) > 0) {
          # upsert column data
          reference_table = meta_column_data[[schema]]
          # novel_data = pg_column_data[[schema]] %>% collect()
          novel_data = meta_column_upsert[[schema]]
          temp_table_name = stage_to_temp(con, reference_table, novel_data)
          pointer = tbl(con, temp_table_name)
          rows_upsert(meta_column_data[[schema]], pointer, by=columns_pkey, in_place=TRUE)
        
          cat("Columns upserted for schema: ", schema, "\n")
        }
      }
      
      # Commit the transaction if successful
      dbCommit(con)
      print("Transaction successful! All tables are up to date.")

    }, error = function(e) {
      # Rollback in case of error
      dbRollback(con)
      message("Transaction failed: ", e$message)
    })
}else{
  print("No new data. All tables are up to date.")
}

```